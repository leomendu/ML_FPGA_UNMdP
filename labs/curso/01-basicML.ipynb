{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../img/general/header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducci칩n al Aprendizaje Autom치tico\n",
    "\n",
    "El objetivo de este laboratorio es introducir los conceptos b치sicos involucrados en el entrenamiento y verificaci칩n de los modelos basados en aprendizaje automa치tico. \n",
    "Para esto, se har치 uso de MNIST y CFAR-10 dataset. \n",
    "Las m칠tricas a obtener incluyen: matriz de confusi칩n, ROC, accuracy. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "游눠 Las redes neuronales est치n compuestas por par치metros e hiperpar치metros. Los par치metros se aprenden durante el entrenamiento, como los pesos de los filtros en una red neuronal convolucional (CNN). En contraste, los hiperpar치metros son definidos por el usuario antes de que comience el entrenamiento. Los hiperpar치metros especifican tanto la arquitectura, como el n칰mero de capas, el n칰mero de filtros por capa y las dimensiones de los filtros, as칤 como el proceso de aprendizaje, incluyendo la tasa de aprendizaje, el tama침o del lote, el n칰mero de 칠pocas y los criterios de detenci칩n temprana.\n",
    "\n",
    "Los principales pasos involucrados en el proceso de entrenamiento y prueba se ilustran en la siguiente imagen, junto con sus funciones correspondientes.\n",
    "\n",
    "Los dos primeros pasos se centran en definir los hiperpar치metros y configurar la arquitectura del aprendizaje autom치tico. Posteriormente, un resumen del modelo ayuda a revisar c칩mo fue construido.\n",
    "\n",
    "Una vez que se crea el modelo, se configuran par치metros como el optimizador, la funci칩n de p칠rdida y las m칠tricas utilizando la funci칩n _model.compile()_.\n",
    "\n",
    "Finalmente, el entrenamiento se lleva a cabo con la funci칩n _model.fit()_, donde se especifican el conjunto de datos, el tama침o del lote (batch), el n칰mero de 칠pocas (epochs) y los callbacks, entre otros.\n",
    "\n",
    "![alt text](../img/lab01/steps_ml.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librer칤as\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "## Tensorflow + Keras libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense,Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "## Datasets\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Habilitaci칩n de GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU \n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"GPUs: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de dataset\n",
    "\n",
    "En este ejercicio se va a emplear MNIST dataset para efectuar el entrenamiento del modelo ML para clasificaci칩n. MNIST est치 compuesto por 10 d칤gitos, del 0 al 9.\n",
    "\n",
    "![alt text](../img/lab01/01_lab_mnist.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del dataset, dividido en trainig y testing (data y clase)\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecci칩n del dataset\n",
    "\n",
    "Visualizaci칩n de los elementos que componen el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indice de la imagen (N) del conjunto de datos.Variando este valor se modifica la imagen a mostrar\n",
    "N = 10\n",
    "\n",
    "# Tama침o de la imagen a mostrar\n",
    "plt.figure(figsize=(4,4))\n",
    "\n",
    "plt.imshow(x_train[N], cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  游닇 Ejercicio: \n",
    "\n",
    "Cambie el valor de **N** para obervar los distintos elementos del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Forma\" de la variable x_train.\n",
    "# Esto es necesario para saber la cantidad de elementos con los que contamos y la dimensi칩n del los mismos.\n",
    "# En este caso, X_train contiene 60000 elementos de 28x28 p칤xeles.\n",
    "# El tama침o de la imagen es importante porque necesitaremos usarlo en la construcci칩n de la arquitectura basada en ML.\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-procesamiento\n",
    "\n",
    "Cuando trabajamos con im치genes, generalmente estas deben someterse a un pre-procesamiento b치sico que corresponde normalizar los valores entre [0, 1], debido a que los p칤xeles que la componen se encuentran en el rango [0, 255]. Esto se logra mediante una simple divisi칩n del valor de los p칤xeles originales por 255.\n",
    "\n",
    "En este caso lo vamos a aplicar a las variables que contienen las im치genes, *x_train* y *x_test*; debido a que *y_train* e *y_test* tienen el valor de la clase correspondiente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_norm, x_test_norm = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquitectura MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenci칩n del ancho y altura de la imagen \n",
    "\n",
    "w = x_train_norm.shape[1]\n",
    "h = x_train_norm.shape[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N칰mero de clases en el dataset\n",
    "n_classes = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definici칩n de la arquitectura basada en MLP \n",
    "\n",
    "model= Sequential([\n",
    "\n",
    "    Flatten(input_shape=(w, h)),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(n_classes, activation='softmax')\n",
    "\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  游닇 Ejercicio: \n",
    "\n",
    "Cu치l es la funci칩n de la capa *flatten*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model compile\n",
    "\n",
    "- **loss:**  m칠trica que mide qu칠 tan lejos est치n las predicciones del modelo de los valores reales.\n",
    "\n",
    "    - sparse_categorical_crossentropy: cuando la etiqueta (o clase) se encuentra en valores enteros. \n",
    "    - categorical_crossentropy: cuando la etiqueta (o clase) se encuentra en valores one-hot encoded. \n",
    "\n",
    "- **optimizer:** algoritmo que ajusta los pesos de la red neuronal para minimizar la funci칩n de p칠rdida.\n",
    "\n",
    "    - SGD (Descenso de gradiente estoc치stico)\n",
    "    - Adam (Adaptative Moment Estimation, balancea velocidad y estabilidad)\n",
    "\n",
    "- **learning rate:** hiperpar치metro que controla qu칠 tan grandes son los ajustes que hace el optimizador en los pesos del modelo en cada iteraci칩n.\n",
    "\n",
    "- **metrics:** valores adicionales que se monitorean durante el entrenamiento para evaluar el desempe침o del modelo. Por ejemplo, accuracy (precisi칩n, usada en clasificaci칩n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "# op = Adam(lr)\n",
    "op = SGD(lr)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=op, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento\n",
    "\n",
    "- **x_train_norm:** conjunto de datos normalizados obtenidos a partir de aplicar una transformaci칩n a x_train.\n",
    "\n",
    "- **y_train:** etiquetas (o valores esperados) correspondientes a los datos de entrenamiento.\n",
    "\n",
    "- **batch:** n칰mero de muestras que se procesan antes de actualizar los pesos del modelo.\n",
    "\n",
    "- **epochs:** n칰mero de veces que el modelo recorrer치 completamente el conjunto de datos de entrenamiento.\n",
    "\n",
    "- **validation_split:** porcentaje del conjunto de datos de entrenamiento (x_train, y_train) que se reserva para validaci칩n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train_norm, y_train, epochs= 32, batch_size = 50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluaci칩n del modelo\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"\\nTest accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr치fico de accuracy over epochs\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy during training')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr치fico de loss over epochs\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss during training')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba de inferencia\n",
    "\n",
    "Una de las pruebas fundamentales para verificar el funcionamiento del modelo entrenado es la prueba por imagen, de manera independiente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index = 12\n",
    "\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(x_test[index], cmap='gray')\n",
    "plt.title(\"Image for inference\")\n",
    "\n",
    "x_test_norm_reshaped = x_test_norm[index].reshape(-1, 28*28)\n",
    "\n",
    "y_pred = model.predict(x_test_norm_reshaped)\n",
    "\n",
    "y_pred = np.argmax(y_pred) \n",
    "print(f\"Predicted Class: {y_pred} - True label: {y_test[index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M칠tricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc, roc_auc_score\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reporte de m칠tricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_classification = classification_report(y_test, y_pred_labels, digits=4) #, output_dict=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report_classification)\n",
    "\n",
    "print('Accuracy avg     %.4f' % test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matriz de confusi칩n\n",
    "\n",
    "La **matriz de confusi칩n** se puede generar para analizar el rendimiento del modelo en t칠rminos de precisi칩n. Presenta los resultados de las predicciones en un formato matricial, comparando los valores reales (etiquetas verdaderas) con los valores predichos. En un problema de clasificaci칩n binaria, la matriz de confusi칩n es una cuadr칤cula de 2x2 con las siguientes partes:\n",
    "\n",
    "- **Verdaderos Positivos (VP)**: N칰mero de instancias en las que el modelo predijo correctamente la clase positiva.\n",
    "\n",
    "- **Verdaderos Negativos (VN)**: N칰mero de instancias en las que el modelo predijo correctamente la clase negativa.\n",
    "\n",
    "- **Falsos Positivos (FP)**: N칰mero de instancias en las que el modelo predijo incorrectamente la clase positiva.\n",
    "\n",
    "- **Falsos Negativos (FN)**: N칰mero de instancias en las que el modelo predijo incorrectamente la clase negativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred_probs = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap = 'Purples')\n",
    "plt.title('Confusion matrix for MNIST dataset - MLP-based model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# compute ROC curve and ROC area for each class\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(10):\n",
    "  fpr[i], tpr[i], _ = roc_curve(y_test, y_pred_probs[:,i], pos_label=i) \n",
    "  roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "## roc_auc_score require the y_test and y_pred array be in catecorical mode\n",
    "ycat_test = keras.utils.to_categorical(y_test, 10)\n",
    "ycat_test_pred = keras.utils.to_categorical(y_pred, 10)\n",
    "\n",
    "print(roc_auc_score(ycat_test, ycat_test_pred, multi_class='ovr', average='weighted'))\n",
    "\n",
    "# Plot with for-loop\n",
    "color = ['blue','orange', 'green', 'red', 'purple', 'pink', 'olive', 'steelblue', 'gold', 'brown' ]\n",
    "plt.figure(figsize=(15,5))\n",
    "for g in range (10):\n",
    "  plt.plot(fpr[g], tpr[g], color=color[g], label='Class %d vs the rest (area = %.2f)' % (g, roc_auc[g]))\n",
    "\n",
    "# Plot by element\n",
    "# plt.plot(fpr[0], tpr[0], color='blue', label='Class 0 vs the rest (area = %.2f)' % roc_auc[0])\n",
    "# plt.plot(fpr[1], tpr[1], color='red', label='Class 1 vs the rest (area = %.2f)' % roc_auc[1])\n",
    "# plt.plot(fpr[2], tpr[2], color='green', label='Class 2 vs the rest (area = %.2f)' % roc_auc[2])\n",
    "# plt.plot(fpr[3], tpr[3], color='orange', label='Class 3 vs the rest (area = %.2f)' % roc_auc[3])\n",
    "\n",
    "plt.title('ROC curve for MNIST dataset')\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  游닇 Ejercicio: \n",
    "\n",
    "Var칤e el valor de batch, epoch, y learning rate. Obtenga las gr치ficas correspondientes. Qu칠 conclusiones puede obtener al variar estos par치metros?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guardar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../models/mnistModel_FC.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uso de One-Hot Encoding\n",
    "\n",
    "El One-Hot Encoding es una t칠cnica para convertir etiquetas categ칩ricas en un formato num칠rico que las redes neuronales pueden procesar.\n",
    "\n",
    "Por ejemplo, para MNIST, tenemos las clases 0, 1, 2, 3 ... hasta 9. En one-hot encoding estas se representan de la siguiente manera: \n",
    "\n",
    "0 -> [1, 0, 0, 0, ...]\n",
    "\n",
    "1 -> [0, 1, 0, 0, ...]\n",
    "\n",
    "2 -> [0, 0, 1, 0, ...]\n",
    "\n",
    "3 -> [0, 0, 0, 1, ...]\n",
    "\n",
    "y asi sucesivamente con todas las categorias presentes en el dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode labels\n",
    "y_train = to_categorical(y_train, num_classes = n_classes)\n",
    "y_test = to_categorical(y_test, num_classes = n_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de categorical_crossentropy loss \n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train_norm, y_train, epochs= 16, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy over epochs\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss over epochs\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### CIFAR-10 para CNN\n",
    "\n",
    "Vamos a repetir el procedimiento anterior pero haciendo uso de CIFAR-10 dataset. En este caso, vamos a emplear una arquitectura badada en 2D-CNN \n",
    "\n",
    "![alt text](../img/lab01/01_lab_cifar10.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del dataset CIFAR-10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tama침o de la imagen a mostrar\n",
    "plt.figure(figsize=(2,2))\n",
    "\n",
    "# La imagen es el elemento N del conjunto de datos \n",
    "N = 100\n",
    "plt.imshow(x_train[N])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizacion del dataset\n",
    "x_train_norm, x_test_norm = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential([\n",
    "\n",
    "    Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(32,32,3)), \n",
    "    MaxPooling2D((2,2)),\n",
    "    \n",
    "    Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2,2)),\n",
    "    \n",
    "    Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2,2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),  # Reduce overfitting\n",
    "    \n",
    "    Dense(10, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar el modelo\n",
    "op = Adam(0.001)\n",
    "model.compile(optimizer=op,\n",
    "              loss='sparse_categorical_crossentropy',   # Usar 'sparse_categorical_crossentropy' porque las etiquetas NO est치n one-hot encoded\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(x_train, y_train, epochs=32,  validation_split=0.2, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el modelo\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar accuracy deurante el entrenamiento\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "####  游닇 Ejercicios: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Computar las m칠tricas para CIFAR-10.\n",
    "\n",
    "- Para MNIST, reemplazar la capa _Flatten_ en la definicion del modelo por _Dense_. Para este paso, la imagen se debe convertir en un vector de 1D. \n",
    "\n",
    "- Repetir el procedimiento anterior par fashion MNIST y CIFAR-100 datasets. \n",
    "\n",
    "- Obtener las m칠tricas pertinentes para verificar el rendimiento del modelo.\n",
    "\n",
    "- Para CIFAR-10 seleccione 2 clases e implemente un clasificador binario.\n",
    "\n",
    "![alt text](../img/lab01/01_lab_fashionmnist.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de Fashion MNIST dataset\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de CIFAR-100 dataset\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
