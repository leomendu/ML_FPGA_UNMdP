{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../img/general/header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducci√≥n al Aprendizaje Autom√°tico\n",
    "\n",
    "El objetivo de este laboratorio es introducir los conceptos b√°sicos involucrados en el entrenamiento y verificaci√≥n de los modelos basados en aprendizaje automa√°tico. \n",
    "Para esto, se har√° uso de MNIST y CFAR-10 dataset. \n",
    "Las m√©tricas a obtener incluyen: matriz de confusi√≥n, ROC, accuracy. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° Las redes neuronales est√°n compuestas por par√°metros e hiperpar√°metros. Los par√°metros se aprenden durante el entrenamiento, como los pesos de los filtros en una red neuronal convolucional (CNN). En contraste, los hiperpar√°metros son definidos por el usuario antes de que comience el entrenamiento. Los hiperpar√°metros especifican tanto la arquitectura, como el n√∫mero de capas, el n√∫mero de filtros por capa y las dimensiones de los filtros, as√≠ como el proceso de aprendizaje, incluyendo la tasa de aprendizaje, el tama√±o del lote, el n√∫mero de √©pocas y los criterios de detenci√≥n temprana.\n",
    "\n",
    "Los principales pasos involucrados en el proceso de entrenamiento y prueba se ilustran en la siguiente imagen, junto con sus funciones correspondientes.\n",
    "\n",
    "Los dos primeros pasos se centran en definir los hiperpar√°metros y configurar la arquitectura del aprendizaje autom√°tico. Posteriormente, un resumen del modelo ayuda a revisar c√≥mo fue construido.\n",
    "\n",
    "Una vez que se crea el modelo, se configuran par√°metros como el optimizador, la funci√≥n de p√©rdida y las m√©tricas utilizando la funci√≥n _model.compile()_.\n",
    "\n",
    "Finalmente, el entrenamiento se lleva a cabo con la funci√≥n _model.fit()_, donde se especifican el conjunto de datos, el tama√±o del lote (batch), el n√∫mero de √©pocas (epochs) y los callbacks, entre otros.\n",
    "\n",
    "![alt text](../img/lab01/steps_ml.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librer√≠as\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "## Tensorflow + Keras libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense,Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "## Datasets\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Habilitaci√≥n de GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU \n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"GPUs: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de dataset\n",
    "\n",
    "En este ejercicio se va a emplear MNIST dataset para efectuar el entrenamiento del modelo ML para clasificaci√≥n. MNIST est√° compuesto por 10 d√≠gitos, del 0 al 9.\n",
    "\n",
    "![alt text](../img/lab01/01_lab_mnist.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del dataset, dividido en trainig y testing (data y clase)\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecci√≥n del dataset\n",
    "\n",
    "Visualizaci√≥n de los elementos que componen el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indice de la imagen (N) del conjunto de datos.Variando este valor se modifica la imagen a mostrar\n",
    "N = 10\n",
    "\n",
    "# Tama√±o de la imagen a mostrar\n",
    "plt.figure(figsize=(4,4))\n",
    "\n",
    "plt.imshow(x_train[N], cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  üìù Ejercicio: \n",
    "\n",
    "Cambie el valor de **N** para obervar los distintos elementos del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Forma\" de la variable x_train.\n",
    "# Esto es necesario para saber la cantidad de elementos con los que contamos y la dimensi√≥n del los mismos.\n",
    "# En este caso, X_train contiene 60000 elementos de 28x28 p√≠xeles.\n",
    "# El tama√±o de la imagen es importante porque necesitaremos usarlo en la construcci√≥n de la arquitectura basada en ML.\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-procesamiento\n",
    "\n",
    "Cuando trabajamos con im√°genes, generalmente estas deben someterse a un pre-procesamiento b√°sico que corresponde normalizar los valores entre [0, 1], debido a que los p√≠xeles que la componen se encuentran en el rango [0, 255]. Esto se logra mediante una simple divisi√≥n del valor de los p√≠xeles originales por 255.\n",
    "\n",
    "En este caso lo vamos a aplicar a las variables que contienen las im√°genes, *x_train* y *x_test*; debido a que *y_train* e *y_test* tienen el valor de la clase correspondiente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_norm, x_test_norm = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquitectura MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenci√≥n del ancho y altura de la imagen \n",
    "\n",
    "w = x_train_norm.shape[1]\n",
    "h = x_train_norm.shape[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N√∫mero de clases en el dataset\n",
    "n_classes = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definici√≥n de la arquitectura basada en MLP \n",
    "\n",
    "model= Sequential([\n",
    "\n",
    "    Flatten(input_shape=(w, h)),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(n_classes, activation='softmax')\n",
    "\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  üìù Ejercicio: \n",
    "\n",
    "Cu√°l es la funci√≥n de la capa *flatten*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model compile\n",
    "\n",
    "- **loss:**  m√©trica que mide qu√© tan lejos est√°n las predicciones del modelo de los valores reales.\n",
    "\n",
    "    - sparse_categorical_crossentropy: cuando la etiqueta (o clase) se encuentra en valores enteros. \n",
    "    - categorical_crossentropy: cuando la etiqueta (o clase) se encuentra en valores one-hot encoded. \n",
    "\n",
    "- **optimizer:** algoritmo que ajusta los pesos de la red neuronal para minimizar la funci√≥n de p√©rdida.\n",
    "\n",
    "    - SGD (Descenso de gradiente estoc√°stico)\n",
    "    - Adam (Adaptative Moment Estimation, balancea velocidad y estabilidad)\n",
    "\n",
    "- **learning rate:** hiperpar√°metro que controla qu√© tan grandes son los ajustes que hace el optimizador en los pesos del modelo en cada iteraci√≥n.\n",
    "\n",
    "- **metrics:** valores adicionales que se monitorean durante el entrenamiento para evaluar el desempe√±o del modelo. Por ejemplo, accuracy (precisi√≥n, usada en clasificaci√≥n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "# op = Adam(lr)\n",
    "op = SGD(lr)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=op, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento\n",
    "\n",
    "- **x_train_norm:** conjunto de datos normalizados obtenidos a partir de aplicar una transformaci√≥n a x_train.\n",
    "\n",
    "- **y_train:** etiquetas (o valores esperados) correspondientes a los datos de entrenamiento.\n",
    "\n",
    "- **batch:** n√∫mero de muestras que se procesan antes de actualizar los pesos del modelo.\n",
    "\n",
    "- **epochs:** n√∫mero de veces que el modelo recorrer√° completamente el conjunto de datos de entrenamiento.\n",
    "\n",
    "- **validation_split:** porcentaje del conjunto de datos de entrenamiento (x_train, y_train) que se reserva para validaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train_norm, y_train, epochs= 32, batch_size = 50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluaci√≥n del modelo\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"\\nTest accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fico de accuracy over epochs\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy during training')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fico de loss over epochs\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss during training')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba de inferencia\n",
    "\n",
    "Una de las pruebas fundamentales para verificar el funcionamiento del modelo entrenado es la prueba por imagen, de manera independiente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index = 12\n",
    "\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(x_test[index], cmap='gray')\n",
    "plt.title(\"Image for inference\")\n",
    "\n",
    "x_test_norm_reshaped = x_test_norm[index].reshape(-1, 28*28)\n",
    "\n",
    "y_pred = model.predict(x_test_norm_reshaped)\n",
    "\n",
    "y_pred = np.argmax(y_pred) \n",
    "print(f\"Predicted Class: {y_pred} - True label: {y_test[index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M√©tricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc, roc_auc_score\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reporte de m√©tricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_classification = classification_report(y_test, y_pred_labels, digits=4) #, output_dict=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report_classification)\n",
    "\n",
    "print('Accuracy avg     %.4f' % test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matriz de confusi√≥n\n",
    "\n",
    "La **matriz de confusi√≥n** se puede generar para analizar el rendimiento del modelo en t√©rminos de precisi√≥n. Presenta los resultados de las predicciones en un formato matricial, comparando los valores reales (etiquetas verdaderas) con los valores predichos. En un problema de clasificaci√≥n binaria, la matriz de confusi√≥n es una cuadr√≠cula de 2x2 con las siguientes partes:\n",
    "\n",
    "- **Verdaderos Positivos (VP)**: N√∫mero de instancias en las que el modelo predijo correctamente la clase positiva.\n",
    "\n",
    "- **Verdaderos Negativos (VN)**: N√∫mero de instancias en las que el modelo predijo correctamente la clase negativa.\n",
    "\n",
    "- **Falsos Positivos (FP)**: N√∫mero de instancias en las que el modelo predijo incorrectamente la clase positiva.\n",
    "\n",
    "- **Falsos Negativos (FN)**: N√∫mero de instancias en las que el modelo predijo incorrectamente la clase negativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred_probs = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap = 'Purples')\n",
    "plt.title('Confusion matrix for MNIST dataset - MLP-based model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# compute ROC curve and ROC area for each class\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(10):\n",
    "  fpr[i], tpr[i], _ = roc_curve(y_test, y_pred_probs[:,i], pos_label=i) \n",
    "  roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "## roc_auc_score require the y_test and y_pred array be in catecorical mode\n",
    "ycat_test = keras.utils.to_categorical(y_test, 10)\n",
    "ycat_test_pred = keras.utils.to_categorical(y_pred, 10)\n",
    "\n",
    "print(roc_auc_score(ycat_test, ycat_test_pred, multi_class='ovr', average='weighted'))\n",
    "\n",
    "# Plot with for-loop\n",
    "color = ['blue','orange', 'green', 'red', 'purple', 'pink', 'olive', 'steelblue', 'gold', 'brown' ]\n",
    "plt.figure(figsize=(15,5))\n",
    "for g in range (10):\n",
    "  plt.plot(fpr[g], tpr[g], color=color[g], label='Class %d vs the rest (area = %.2f)' % (g, roc_auc[g]))\n",
    "\n",
    "# Plot by element\n",
    "# plt.plot(fpr[0], tpr[0], color='blue', label='Class 0 vs the rest (area = %.2f)' % roc_auc[0])\n",
    "# plt.plot(fpr[1], tpr[1], color='red', label='Class 1 vs the rest (area = %.2f)' % roc_auc[1])\n",
    "# plt.plot(fpr[2], tpr[2], color='green', label='Class 2 vs the rest (area = %.2f)' % roc_auc[2])\n",
    "# plt.plot(fpr[3], tpr[3], color='orange', label='Class 3 vs the rest (area = %.2f)' % roc_auc[3])\n",
    "\n",
    "plt.title('ROC curve for MNIST dataset')\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  üìù Ejercicio: \n",
    "\n",
    "Var√≠e el valor de batch, epoch, y learning rate. Obtenga las gr√°ficas correspondientes. Qu√© conclusiones puede obtener al variar estos par√°metros?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guardar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../models/mnistModel_FC.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uso de One-Hot Encoding\n",
    "\n",
    "El One-Hot Encoding es una t√©cnica para convertir etiquetas categ√≥ricas en un formato num√©rico que las redes neuronales pueden procesar.\n",
    "\n",
    "Por ejemplo, para MNIST, tenemos las clases 0, 1, 2, 3 ... hasta 9. En one-hot encoding estas se representan de la siguiente manera: \n",
    "\n",
    "0 -> [1, 0, 0, 0, ...]\n",
    "\n",
    "1 -> [0, 1, 0, 0, ...]\n",
    "\n",
    "2 -> [0, 0, 1, 0, ...]\n",
    "\n",
    "3 -> [0, 0, 0, 1, ...]\n",
    "\n",
    "y asi sucesivamente con todas las categorias presentes en el dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode labels\n",
    "y_train = to_categorical(y_train, num_classes = n_classes)\n",
    "y_test = to_categorical(y_test, num_classes = n_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de categorical_crossentropy loss \n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train_norm, y_train, epochs= 16, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy over epochs\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss over epochs\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### CIFAR-10 para CNN\n",
    "\n",
    "Vamos a repetir el procedimiento anterior pero haciendo uso de CIFAR-10 dataset. En este caso, vamos a emplear una arquitectura badada en 2D-CNN \n",
    "\n",
    "![alt text](../img/lab01/01_lab_cifar10.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del dataset CIFAR-10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tama√±o de la imagen a mostrar\n",
    "plt.figure(figsize=(2,2))\n",
    "\n",
    "# La imagen es el elemento N del conjunto de datos \n",
    "N = 100\n",
    "plt.imshow(x_train[N])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizacion del dataset\n",
    "x_train_norm, x_test_norm = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential([\n",
    "\n",
    "    Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(32,32,3)), \n",
    "    MaxPooling2D((2,2)),\n",
    "    \n",
    "    Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2,2)),\n",
    "    \n",
    "    Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2,2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),  # Reduce overfitting\n",
    "    \n",
    "    Dense(10, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar el modelo\n",
    "op = Adam(0.001)\n",
    "model.compile(optimizer=op,\n",
    "              loss='sparse_categorical_crossentropy',   # Usar 'sparse_categorical_crossentropy' porque las etiquetas NO est√°n one-hot encoded\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(x_train, y_train, epochs=32,  validation_split=0.2, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el modelo\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar accuracy deurante el entrenamiento\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "####  üìù Ejercicios: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Computar las m√©tricas para CIFAR-10.\n",
    "\n",
    "- Para MNIST, reemplazar la capa _Flatten_ en la definicion del modelo por _Dense_. Para este paso, la imagen se debe convertir en un vector de 1D. \n",
    "\n",
    "- Repetir el procedimiento anterior par fashion MNIST y CIFAR-100 datasets. \n",
    "\n",
    "- Obtener las m√©tricas pertinentes para verificar el rendimiento del modelo.\n",
    "\n",
    "- Para CIFAR-10 seleccione 2 clases e implemente un clasificador binario.\n",
    "\n",
    "![alt text](../img/lab01/01_lab_fashionmnist.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de Fashion MNIST dataset\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de CIFAR-100 dataset\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
